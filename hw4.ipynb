{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d3a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# from train import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ded0e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7faeb3faab80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = KeyedVectors.load('embs_train.kv')\n",
    "wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2d70f",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f7695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.7597668170928955),\n",
       " ('terrible', 0.7478912472724915),\n",
       " ('dreadful', 0.7218177914619446),\n",
       " ('horrid', 0.6720177531242371),\n",
       " ('atrocious', 0.6626645922660828),\n",
       " ('ugly', 0.6236302256584167),\n",
       " ('lousy', 0.6135217547416687),\n",
       " ('unbelievable', 0.6068726181983948),\n",
       " ('appalling', 0.6061565279960632),\n",
       " ('hideous', 0.5811460614204407)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('awful', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3ef508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('students', 0.7294867038726807),\n",
       " ('teacher', 0.6301366090774536),\n",
       " ('school', 0.6055627465248108),\n",
       " ('undergraduate', 0.6020305752754211),\n",
       " ('university', 0.600540041923523),\n",
       " ('campus', 0.5629045367240906),\n",
       " ('academic', 0.5484224557876587),\n",
       " ('professors', 0.530981183052063),\n",
       " ('college', 0.525564968585968),\n",
       " ('grad', 0.5203014016151428)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('student', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa56cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relatives', 0.6662653088569641),\n",
       " ('families', 0.6252894997596741),\n",
       " ('siblings', 0.6140849590301514),\n",
       " ('friends', 0.6128394603729248),\n",
       " ('mother', 0.6065612435340881),\n",
       " ('father', 0.5717043876647949),\n",
       " ('wife', 0.5601866245269775),\n",
       " ('son', 0.5384211540222168),\n",
       " ('clan', 0.5372899770736694),\n",
       " ('grandmother', 0.5366827845573425)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('family', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0f749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('successes', 0.724018394947052),\n",
       " ('successful', 0.6167578101158142),\n",
       " ('accomplishment', 0.49159350991249084),\n",
       " ('achievements', 0.4895521104335785),\n",
       " ('achievement', 0.4850277304649353),\n",
       " ('triumphs', 0.4617437720298767),\n",
       " ('greatness', 0.45542895793914795),\n",
       " ('progress', 0.44958776235580444),\n",
       " ('popularity', 0.4400866627693176),\n",
       " ('triumph', 0.43484923243522644)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('success', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544a3047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enhancing', 0.7954033613204956),\n",
       " ('improve', 0.7549501657485962),\n",
       " ('enhances', 0.7072708010673523),\n",
       " ('enhanced', 0.6955755352973938),\n",
       " ('bolster', 0.6009522080421448),\n",
       " ('elevate', 0.5607604384422302),\n",
       " ('enable', 0.5488600134849548),\n",
       " ('boost', 0.5353639721870422),\n",
       " ('reduce', 0.5311101675033569),\n",
       " ('fortify', 0.5228351354598999)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('enhance', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c7f571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brother', 0.7966989874839783),\n",
       " ('uncle', 0.6753759980201721),\n",
       " ('nephew', 0.6596081852912903),\n",
       " ('son', 0.6472460031509399),\n",
       " ('father', 0.6398823261260986)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['sister', 'man'], negative=['woman'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11f5087b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faster', 0.7064898610115051),\n",
       " ('rapidly', 0.5021133422851562),\n",
       " ('easier', 0.48843103647232056),\n",
       " ('slow', 0.45752349495887756),\n",
       " ('quickly', 0.4370785653591156)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['harder', 'fast'], negative=['hard'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a83faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.5181134343147278)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a7433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('summer', 0.5669187903404236),\n",
       " ('spring', 0.5186282396316528),\n",
       " ('hottest', 0.5085405707359314),\n",
       " ('summertime', 0.4723301827907562),\n",
       " ('season', 0.4058019518852234)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['winter', 'hot'], negative=['cold'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f5e0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laws', 0.4727955460548401),\n",
       " ('lawyer', 0.4094833433628082),\n",
       " ('judge', 0.38079601526260376),\n",
       " ('legally', 0.3534584939479828),\n",
       " ('dentist', 0.34965550899505615)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['doctor', 'law'], negative=['medicine'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f14c2",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f32ff1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'train.txt'\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract labels and texts\n",
    "    data = {'Label': [1 if line.startswith('+') else 0 for line in lines],\n",
    "            'Text': [line[2:].strip() for line in lines]}  # Adjust slicing if necessary\n",
    "\n",
    "    new_df = pd.DataFrame(data)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3decab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train.txt'\n",
    "train_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b7f86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dev.txt'\n",
    "dev_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c306f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "test_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "710faa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['Label']\n",
    "train_X = train_df['Text']\n",
    "\n",
    "dev_y = dev_df['Label']\n",
    "dev_X = dev_df['Text']\n",
    "\n",
    "test_X = test_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bca2552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0] == train_df.iloc[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1ce5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence to a vector\n",
    "def sentence_to_vector(sentence, wv):\n",
    "    words = sentence.lower().split()\n",
    "    valid_embeddings = [wv[word] for word in words if word in wv]\n",
    "    if not valid_embeddings:\n",
    "        return np.zeros(wv.vector_size)\n",
    "    return np.mean(valid_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "035bc4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_vec(data):\n",
    "    sentence_vectors = []\n",
    "    for sentence in data:\n",
    "        vector = sentence_to_vector(sentence, wv)\n",
    "        sentence_vectors.append(vector)\n",
    "\n",
    "    # Convert the list of sentence vectors to a NumPy array\n",
    "    sentence_vectors = np.array(sentence_vectors)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "556d798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vector = sentence_vec(train_X)\n",
    "x_dev_vector =  sentence_vec(dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29c8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = x_train_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab8b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(sentence, sentence_vectors, feature, main_df, number):\n",
    "    similarities = []\n",
    "    label = None\n",
    "    \n",
    "    # Loop through each vector in sentence_vectors\n",
    "    for sentence_vector in sentence_vectors:\n",
    "        # Calculate cosine similarity with the first sentence\n",
    "        similarity = cosine_similarity([sentence], [sentence_vector])\n",
    "        # Append the similarity score to the list (accessing the first element as similarity returns a 2D array)\n",
    "        similarities.append(similarity[0][0])\n",
    "        \n",
    "    similarities[number] = -1\n",
    "\n",
    "    closest_sentence_idx = np.argmax(similarities)\n",
    "\n",
    "    # Closest sentence\n",
    "    closest_sentence = feature[closest_sentence_idx]\n",
    "    \n",
    "    target_sentence = closest_sentence\n",
    "    for index, row in main_df.iterrows():\n",
    "        sentence = row['Text']  # Replace 'Sentence' with your actual column name for sentences\n",
    "        label = row['Label']        # Replace 'Label' with your actual column name for labels\n",
    "\n",
    "        if sentence == target_sentence:\n",
    "            if label == 0:\n",
    "                label = \"-\"\n",
    "            else:\n",
    "                label = '+'\n",
    "            break\n",
    "\n",
    "    \n",
    "    print(f\"Main senetence: {feature[number]}.\")\n",
    "    print(f\"Closest sentence label: {label}\")\n",
    "    print(f\"Closest sentence: {closest_sentence}.\")\n",
    "    \n",
    "    return closest_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f04049",
   "metadata": {},
   "source": [
    "### 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6a1da4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main senetence: it 's a tour de force , written and directed so quietly that it 's implosion rather than explosion you fear.\n",
      "Closest sentence label: -\n",
      "Closest sentence: a semi autobiographical film that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the caterer had anything to do with it.\n"
     ]
    }
   ],
   "source": [
    "first_similarities = find_similarity(first_sentence, x_train_vector, train_X, train_df, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198bb9f",
   "metadata": {},
   "source": [
    "### 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a5ecaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sentence = x_train_vector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce16abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main senetence: places a slightly believable love triangle in a difficult to swallow setting , and then disappointingly moves the story into the realm of an improbable thriller.\n",
      "Closest sentence label: -\n",
      "Closest sentence: the plan to make enough into an inspiring tale of survival wrapped in the heart pounding suspense of a stylish psychological thriller ' has flopped as surely as a souffl gone wrong.\n"
     ]
    }
   ],
   "source": [
    "second_similarities = find_similarity(second_sentence, x_train_vector, train_X, train_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "009f5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(X_train, y_train, X_dev, y_dev):\n",
    "    error_rates = {}\n",
    "\n",
    "    for k in range(1, 100, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        dev_predictions = knn.predict(X_dev)\n",
    "\n",
    "        accuracy = accuracy_score(y_dev, dev_predictions)\n",
    "\n",
    "        error_rate = 1 - accuracy\n",
    "\n",
    "        error_rates[k] = [error_rate, knn]\n",
    "   \n",
    "    min_key = min(error_rates, key=lambda k: error_rates[k][0])\n",
    "    print(f\"Best k: {min_key}, smallest dev error: {error_rates[min_key][0]}\")\n",
    "    \n",
    "    return error_rates[min_key][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc0850",
   "metadata": {},
   "source": [
    "### 2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46a6d05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 73, smallest dev error: 0.278\n"
     ]
    }
   ],
   "source": [
    "best_knn = knn_classifier(x_train_vector, train_y, x_dev_vector, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c9b02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_vector = sentence_vec(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bebf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = best_knn.predict(x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53aebd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pd.DataFrame(predicted, columns=['Text'])\n",
    "# predictions_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4e860",
   "metadata": {},
   "source": [
    "### 2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feaed117",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hot = train_df['Label']\n",
    "train_X_hot = train_df['Text']\n",
    "\n",
    "dev_y_hot = dev_df['Label']\n",
    "dev_X_hot = dev_df['Text']\n",
    "\n",
    "test_X_hot = test_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f3e65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and conversion to numerical feature vectors using Bag-of-Words\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "train_X_vectorized = vectorizer.fit_transform(train_X_hot)\n",
    "train_X_vectorized_dense = train_X_vectorized.toarray()\n",
    "\n",
    "dev_X_vectorized = vectorizer.transform(dev_df['Text'])\n",
    "# Convert the sparse matrix to a dense array\n",
    "dev_X_vectorized_dense = dev_X_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "504baaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['007', '10', '100', ..., 'zoolander', 'zucker', 'zwick'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6815b466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 15, smallest dev error: 0.383\n"
     ]
    }
   ],
   "source": [
    "best_knn = knn_classifier(train_X_vectorized_dense, train_y_hot, dev_X_vectorized_dense, dev_y_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0447d",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "255b7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import time\n",
    "# from svector import svector\n",
    "\n",
    "def read_from(textfile):\n",
    "    for line in open(textfile):\n",
    "        label, words = line.strip().split(\"\\t\")\n",
    "        yield (1 if label==\"+\" else -1, words.split())\n",
    "    \n",
    "def test(devfile, model, word_vectors):\n",
    "    tot, err = 0, 0\n",
    "    for i, (label, words) in enumerate(read_from(devfile), 1): # note 1...|D|\n",
    "        sentence_embedding = make_vector(words, word_vectors)\n",
    "        prediction = np.dot(model, sentence_embedding)\n",
    "        err += label * prediction <= 0\n",
    "    return err/i  # i is |D| now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "624f20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(words, word_vectors):\n",
    "    embeddings = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis = 0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "822b805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_basic(trainfile, devfile, word_vectors, epochs=10):\n",
    "    t = time.time()\n",
    "    best_err = 1.\n",
    "    model = np.zeros(word_vectors.vector_size)\n",
    "    best_model = model.copy()\n",
    "    for it in range(1, epochs+1):\n",
    "        updates = 0\n",
    "        for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "            sent = make_vector(words, word_vectors)\n",
    "            if label * np.dot(model, sent) <= 0:\n",
    "                updates += 1\n",
    "                model += label * sent\n",
    "        dev_err = test(devfile, model, word_vectors)\n",
    "        if dev_err < best_err:\n",
    "            best_model = model.copy()\n",
    "            best_err = dev_err\n",
    "        print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "    print(\"best dev err %.1f%%, |w|=%d, time: %.1f secs\" % (best_err * 100, len(model), time.time() - t))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f87b6fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, update 31.1%, dev 37.4%\n",
      "epoch 2, update 29.5%, dev 35.4%\n",
      "epoch 3, update 29.8%, dev 33.2%\n",
      "epoch 4, update 29.1%, dev 40.0%\n",
      "epoch 5, update 29.7%, dev 35.2%\n",
      "epoch 6, update 29.4%, dev 40.4%\n",
      "epoch 7, update 29.4%, dev 38.4%\n",
      "epoch 8, update 29.4%, dev 42.5%\n",
      "epoch 9, update 29.1%, dev 39.0%\n",
      "epoch 10, update 29.1%, dev 39.2%\n",
      "best dev err 33.2%, |w|=300, time: 8.7 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.00598490e+00,  2.34847342e-01,  1.34461514e+00,  8.38283854e-01,\n",
       "        7.76929314e-01, -2.16661256e+00, -1.76399479e+00,  4.66723936e-01,\n",
       "       -1.35334088e-01, -6.64555523e-01, -6.87830750e-01, -2.71037435e+00,\n",
       "       -1.34277877e+00, -4.96574975e-01, -4.87512803e-01, -2.46967947e-01,\n",
       "       -6.17474215e-01, -4.65677193e-01, -7.64027532e-01,  2.95568793e-01,\n",
       "        4.84064350e-01, -4.70414176e-01,  2.75841371e+00,  2.33994782e-01,\n",
       "       -1.22336734e-01, -5.43849311e-01, -6.48343692e-01,  6.97254429e-01,\n",
       "       -1.94000242e+00, -4.38251254e-01, -1.01712185e+00,  2.31187137e-01,\n",
       "       -6.24905476e-01, -7.04480509e-01,  3.87558706e-01,  6.90070035e-01,\n",
       "       -6.10672049e-01, -5.86942534e-01,  4.40280042e-03,  1.35342819e+00,\n",
       "        1.22522119e+00,  2.11149208e-01, -9.80365908e-01,  1.48128555e+00,\n",
       "        2.34985066e+00,  9.13465053e-01,  4.98046289e-01, -2.84336815e-01,\n",
       "        8.32202857e-01,  1.04895266e+00, -8.58216584e-01, -1.82093867e+00,\n",
       "       -2.32548645e-01, -1.05769471e+00, -6.21068390e-01,  2.00944071e+00,\n",
       "       -2.22053673e-01, -5.31417225e-01, -3.79003972e-01,  1.08553139e+00,\n",
       "       -1.19358890e-01, -1.02658655e+00, -1.21539195e+00, -1.05661760e-01,\n",
       "        6.61443463e-01, -1.97260458e-01, -2.01538874e-01,  5.50373782e-02,\n",
       "        9.79188990e-01,  1.67128635e+00,  1.29396891e+00,  3.59601025e-01,\n",
       "        2.08872843e-01, -6.82535415e-01,  2.19113785e-01, -8.98595966e-01,\n",
       "        7.22442132e-01,  1.08962566e+00, -3.02894946e-01,  1.64289357e-01,\n",
       "       -1.45625386e+00,  1.56966524e+00, -3.17926781e-01,  2.21374387e+00,\n",
       "       -8.53154429e-01,  5.83958875e-02, -5.43383698e-01, -8.54851844e-01,\n",
       "       -1.02155641e+00,  1.42056907e+00,  5.29212449e-01,  7.43330133e-01,\n",
       "       -5.10882931e-01,  1.43216604e+00, -1.19882828e+00,  8.19260316e-02,\n",
       "       -3.54822211e-01, -6.50306703e-01, -8.56463059e-01, -1.56624428e+00,\n",
       "       -4.91556407e-01,  1.74756552e+00, -4.17046219e-01, -8.98041416e-01,\n",
       "        1.37339493e-01, -9.21890147e-01, -5.15290383e-01,  6.33779120e-01,\n",
       "        1.33797396e+00, -8.46419639e-01, -5.31195096e-01,  1.62558129e+00,\n",
       "       -1.60363005e+00,  8.97837913e-01, -7.69959396e-01,  9.03007551e-01,\n",
       "       -5.28691950e-01,  2.72072384e-01, -7.14733356e-01, -1.04827630e+00,\n",
       "        1.08789644e+00,  1.32144683e+00,  1.76081063e+00, -2.04658170e+00,\n",
       "        1.25584935e+00,  1.37714606e+00,  3.35066701e-01,  1.78218467e+00,\n",
       "       -1.58623032e-01,  9.76891205e-01, -1.80528460e+00,  3.02827004e-01,\n",
       "        2.38613819e-01,  8.44264112e-01,  8.84568686e-02, -7.15077178e-01,\n",
       "        2.34052631e+00,  1.52548296e+00, -9.39210584e-02,  1.86571119e+00,\n",
       "        1.35893773e-02,  8.36447026e-01, -8.91919788e-01, -7.66838163e-01,\n",
       "       -1.09470940e+00,  6.19429621e-01,  1.12210364e+00, -2.00931422e-01,\n",
       "        7.46613584e-02,  2.01796298e+00, -6.69940998e-01, -1.02149221e+00,\n",
       "        6.25811867e-01, -2.96898900e-02, -1.40252632e+00, -1.50297383e+00,\n",
       "       -6.27504550e-01, -1.48271587e-01,  2.30785419e-01, -1.66535890e-01,\n",
       "        5.68630257e-01, -1.35019252e+00, -1.61739778e-01, -6.06342765e-01,\n",
       "       -2.88362071e-01, -1.21838088e+00,  9.69493449e-01,  3.07748193e-01,\n",
       "       -7.95845169e-01,  1.28122860e-01, -2.38937530e-04,  2.08368170e-01,\n",
       "        1.45982656e+00,  6.52450175e-02,  3.19103658e-01, -1.61135191e-01,\n",
       "       -2.43981043e+00,  9.53405688e-01, -5.46785293e-01, -1.18173854e+00,\n",
       "       -4.40023806e-01,  2.88652228e-01,  4.22302430e-01, -3.48668526e+00,\n",
       "       -7.70614342e-01, -1.88376545e+00,  1.31857534e+00, -1.26232654e+00,\n",
       "       -5.04177860e-01, -5.57179349e-01,  5.72530848e-01,  3.02045230e+00,\n",
       "        9.43346693e-01, -2.44802566e+00, -7.41391665e-01, -7.34134333e-01,\n",
       "       -9.73282056e-01,  1.27129161e+00,  6.41607199e-02,  8.93353492e-01,\n",
       "        8.37431506e-01, -9.33779009e-01,  7.26254868e-01, -2.33661097e+00,\n",
       "       -2.73351190e-01, -1.16305909e+00, -1.81599550e-01, -9.46518106e-01,\n",
       "        1.63857747e+00, -3.91325759e-01, -1.36302439e+00,  1.52813465e+00,\n",
       "       -1.33854445e+00, -1.32012225e-01, -7.33958782e-01, -2.23022674e+00,\n",
       "       -1.07071899e+00,  2.11692243e+00, -4.96714311e-01,  1.53838553e+00,\n",
       "        1.57832668e+00, -1.49180800e-01,  2.67788284e-01, -8.03955964e-02,\n",
       "       -1.63626966e+00, -9.86530962e-01, -4.92297079e-01, -7.04988181e-01,\n",
       "        3.76110847e-01,  3.13926358e-01, -1.12909673e+00,  7.59071755e-02,\n",
       "       -1.77359361e-01, -2.43557501e-01,  1.98593757e-01, -1.27990710e+00,\n",
       "       -2.11860656e+00,  3.54690942e-01, -4.08177988e-01,  4.97558176e-01,\n",
       "       -4.67266411e-01, -4.26621285e-01, -1.12508254e+00, -5.51746151e-01,\n",
       "       -5.15821937e-01,  3.33308993e-01, -9.14117709e-02, -1.22396590e+00,\n",
       "        3.84369224e-01, -2.19559377e+00, -8.49448246e-01, -6.01301439e-02,\n",
       "        2.80612041e-01,  1.70216081e+00,  3.98598649e-01,  2.05336522e-01,\n",
       "        6.72196529e-01,  8.69681701e-01,  7.74028122e-01,  1.58424766e+00,\n",
       "        2.07931412e+00, -2.64953081e+00, -8.54027066e-01,  4.16970442e-01,\n",
       "       -9.09575681e-01,  1.32108648e-01, -1.23659656e+00,  4.61464798e-01,\n",
       "        4.90717685e-01, -4.44463101e-01, -1.60871950e+00,  4.64958803e-01,\n",
       "        5.95028109e-01,  5.30413033e-02,  5.27325852e-01,  1.84012366e-01,\n",
       "        1.71847074e+00,  8.96240916e-01, -8.02235935e-01, -9.18265820e-02,\n",
       "        1.09100457e+00, -1.39157810e+00, -2.86250485e+00, -7.38131778e-01,\n",
       "        3.19086995e-01,  3.58306141e-01,  1.33069427e+00,  8.27048249e-01,\n",
       "       -2.99380043e-02,  1.38374689e-02, -8.07728404e-01,  1.13692365e+00,\n",
       "        1.52391927e+00, -2.84374165e+00,  8.87163566e-01,  9.31070834e-01,\n",
       "       -1.15548723e+00, -7.11302136e-01,  1.67228957e+00,  9.12755726e-01])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_basic('train.txt', 'dev.txt', wv, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adff0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_average(trainfile, devfile, word_vectors, epochs=10):\n",
    "    t = time.time()\n",
    "    best_err = 1.\n",
    "    model = np.zeros(word_vectors.vector_size)\n",
    "    avg_model = np.zeros(word_vectors.vector_size)\n",
    "    c = 1\n",
    "    for it in range(1, epochs+1):\n",
    "        updates = 0\n",
    "        for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "            sent = make_vector(words, word_vectors)\n",
    "            if label * np.dot(model, sent) <= 0:\n",
    "                updates += 1\n",
    "                update = label * sent\n",
    "                model += update\n",
    "                avg_model += c * update\n",
    "            c += 1\n",
    "        new_model = model - avg_model/c\n",
    "        dev_err = test(devfile, new_model, word_vectors)\n",
    "        if dev_err < best_err:\n",
    "            best_model = new_model.copy()\n",
    "            best_er = dev_err\n",
    "            \n",
    "        print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "    print(\"best dev err %.1f%%, |w|=%d, time: %.1f secs\" % (best_er * 100, len(best_model), time.time() - t))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f107dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, update 31.1%, dev 24.9%\n",
      "epoch 2, update 29.5%, dev 23.9%\n",
      "epoch 3, update 29.8%, dev 24.3%\n",
      "epoch 4, update 29.1%, dev 24.1%\n",
      "epoch 5, update 29.7%, dev 24.2%\n",
      "epoch 6, update 29.4%, dev 23.9%\n",
      "epoch 7, update 29.4%, dev 23.6%\n",
      "epoch 8, update 29.4%, dev 23.8%\n",
      "epoch 9, update 29.1%, dev 24.1%\n",
      "epoch 10, update 29.1%, dev 24.4%\n",
      "best dev err 24.4%, |w|=300, time: 11.4 secs\n"
     ]
    }
   ],
   "source": [
    "# smart average perceptron\n",
    "model = smart_average('train.txt', 'dev.txt', wv, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6512f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(trainfile, devfile, epochs=5):\n",
    "#     t = time.time()\n",
    "#     best_err = 1.\n",
    "#     model = svector()\n",
    "#     for it in range(1, epochs+1):\n",
    "#         updates = 0\n",
    "#         for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "#             sent = make_vector(words)\n",
    "#             if label * (model.dot(sent)) <= 0:\n",
    "#                 updates += 1\n",
    "#                 model += label * sent\n",
    "#         dev_err = test(devfile, model)\n",
    "#         best_err = min(best_err, dev_err)\n",
    "#         print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "#     print(\"best dev err %.1f%%, |w|=%d, time: %.1f secs\" % (best_err * 100, len(model), time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0121c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt.predicted', 'w') as f:\n",
    "    for i, (label, words) in enumerate(read_from(\"test.txt\"), 1):\n",
    "        f.write(\"%s\\t%s\\n\" % (\"+\" if np.dot(model, make_vector(words, wv)) > 0 else \"-\", \" \".join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c959d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb2a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2124f9",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd2324f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final = train_df['Label']\n",
    "train_X_final = train_df['Text']\n",
    "\n",
    "dev_y_final = dev_df['Label']\n",
    "dev_X_final = dev_df['Text']\n",
    "\n",
    "test_X_final = test_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f709245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to numerical data\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "train_X_final_vectors = tfidf_vectorizer.fit_transform(train_X_final)\n",
    "\n",
    "dev_X_final_vectors = tfidf_vectorizer.transform(dev_X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a652b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression dev error: 0.274\n",
      "Logistic Regression runtime: 0.34636473655700684 seconds\n",
      "Decision Tree dev error: 0.42700000000000005\n",
      "Decision Tree runtime: 3.1895549297332764 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train_X_final_vectors, train_y_final)\n",
    "\n",
    "logistic_predictions = logistic_model.predict(dev_X_final_vectors)\n",
    "\n",
    "logistic_accuracy = accuracy_score(dev_y_final, logistic_predictions)\n",
    "\n",
    "logistic_error = 1 - logistic_accuracy\n",
    "\n",
    "end_time = time.time()\n",
    "logistic_runtime = end_time - start_time\n",
    "print(f'Logistic Regression dev error: {logistic_error}')\n",
    "print(f'Logistic Regression runtime: {logistic_runtime} seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(train_X_final_vectors, train_y_final)\n",
    "\n",
    "tree_predictions = tree_model.predict(dev_X_final_vectors)\n",
    "\n",
    "tree_accuracy = accuracy_score(dev_y_final, tree_predictions)\n",
    "\n",
    "tree_error = 1 - tree_accuracy\n",
    "\n",
    "end_time = time.time()\n",
    "tree_runtime = end_time - start_time\n",
    "print(f'Decision Tree dev error: {tree_error}')\n",
    "print(f'Decision Tree runtime: {tree_runtime} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51980d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f873a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39a6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
