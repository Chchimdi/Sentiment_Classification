{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# from train import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded0e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load('embs_train.kv')\n",
    "wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2d70f",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f7695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.7597668170928955),\n",
       " ('terrible', 0.7478912472724915),\n",
       " ('dreadful', 0.7218177914619446),\n",
       " ('horrid', 0.6720177531242371),\n",
       " ('atrocious', 0.6626645922660828),\n",
       " ('ugly', 0.6236302256584167),\n",
       " ('lousy', 0.6135217547416687),\n",
       " ('unbelievable', 0.6068726181983948),\n",
       " ('appalling', 0.6061565279960632),\n",
       " ('hideous', 0.5811460614204407)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('awful', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3ef508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('students', 0.7294867038726807),\n",
       " ('teacher', 0.6301366090774536),\n",
       " ('school', 0.6055627465248108),\n",
       " ('undergraduate', 0.6020305752754211),\n",
       " ('university', 0.600540041923523),\n",
       " ('campus', 0.5629045367240906),\n",
       " ('academic', 0.5484224557876587),\n",
       " ('professors', 0.530981183052063),\n",
       " ('college', 0.525564968585968),\n",
       " ('grad', 0.5203014016151428)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('student', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa56cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relatives', 0.6662653088569641),\n",
       " ('families', 0.6252894997596741),\n",
       " ('siblings', 0.6140849590301514),\n",
       " ('friends', 0.6128394603729248),\n",
       " ('mother', 0.6065612435340881),\n",
       " ('father', 0.5717043876647949),\n",
       " ('wife', 0.5601866245269775),\n",
       " ('son', 0.5384211540222168),\n",
       " ('clan', 0.5372899770736694),\n",
       " ('grandmother', 0.5366827845573425)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('family', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0f749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('successes', 0.724018394947052),\n",
       " ('successful', 0.6167578101158142),\n",
       " ('accomplishment', 0.49159350991249084),\n",
       " ('achievements', 0.4895521104335785),\n",
       " ('achievement', 0.4850277304649353),\n",
       " ('triumphs', 0.4617437720298767),\n",
       " ('greatness', 0.45542895793914795),\n",
       " ('progress', 0.44958776235580444),\n",
       " ('popularity', 0.4400866627693176),\n",
       " ('triumph', 0.43484923243522644)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('success', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544a3047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enhancing', 0.7954033613204956),\n",
       " ('improve', 0.7549501657485962),\n",
       " ('enhances', 0.7072708010673523),\n",
       " ('enhanced', 0.6955755352973938),\n",
       " ('bolster', 0.6009522080421448),\n",
       " ('elevate', 0.5607604384422302),\n",
       " ('enable', 0.5488600134849548),\n",
       " ('boost', 0.5353639721870422),\n",
       " ('reduce', 0.5311101675033569),\n",
       " ('fortify', 0.5228351354598999)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('enhance', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c7f571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brother', 0.7966989874839783),\n",
       " ('uncle', 0.6753759980201721),\n",
       " ('nephew', 0.6596081852912903),\n",
       " ('son', 0.6472460031509399),\n",
       " ('father', 0.6398823261260986)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['sister', 'man'], negative=['woman'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f5087b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faster', 0.7064898610115051),\n",
       " ('rapidly', 0.5021133422851562),\n",
       " ('easier', 0.48843103647232056),\n",
       " ('slow', 0.45752349495887756),\n",
       " ('quickly', 0.4370785653591156)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['harder', 'fast'], negative=['hard'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a83faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.5181134343147278)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a7433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('summer', 0.5669187903404236),\n",
       " ('spring', 0.5186282396316528),\n",
       " ('hottest', 0.5085405707359314),\n",
       " ('summertime', 0.4723301827907562),\n",
       " ('season', 0.4058019518852234)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['winter', 'hot'], negative=['cold'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f5e0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laws', 0.4727955460548401),\n",
       " ('lawyer', 0.4094833433628082),\n",
       " ('judge', 0.38079601526260376),\n",
       " ('legally', 0.3534584939479828),\n",
       " ('dentist', 0.34965550899505615)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wv.most_similar(positive=['doctor', 'law'], negative=['medicine'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f14c2",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f32ff1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'train.txt'\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract labels and texts\n",
    "    data = {'Label': [1 if line.startswith('+') else 0 for line in lines],\n",
    "            'Text': [line[2:].strip() for line in lines]}  # Adjust slicing if necessary\n",
    "\n",
    "    new_df = pd.DataFrame(data)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3decab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train.txt'\n",
    "train_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b7f86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dev.txt'\n",
    "dev_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c306f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "test_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "710faa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['Label']\n",
    "train_X = train_df['Text']\n",
    "\n",
    "dev_y = dev_df['Label']\n",
    "dev_X = dev_df['Text']\n",
    "\n",
    "test_X = test_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bca2552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0] == train_df.iloc[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1ce5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence to a vector\n",
    "def sentence_to_vector(sentence, wv):\n",
    "    words = sentence.lower().split()\n",
    "    valid_embeddings = [wv[word] for word in words if word in wv]\n",
    "    if not valid_embeddings:\n",
    "        return np.zeros(wv.vector_size)\n",
    "    return np.mean(valid_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "035bc4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_vec(data):\n",
    "    sentence_vectors = []\n",
    "    for sentence in data:\n",
    "        vector = sentence_to_vector(sentence, wv)\n",
    "        sentence_vectors.append(vector)\n",
    "\n",
    "    # Convert the list of sentence vectors to a NumPy array\n",
    "    sentence_vectors = np.array(sentence_vectors)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "556d798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vector = sentence_vec(train_X)\n",
    "x_dev_vector =  sentence_vec(dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29c8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = x_train_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab8b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(sentence, sentence_vectors, feature, main_df, number):\n",
    "    similarities = []\n",
    "    label = None\n",
    "    \n",
    "    # Loop through each vector in sentence_vectors\n",
    "    for sentence_vector in sentence_vectors:\n",
    "        # Calculate cosine similarity with the first sentence\n",
    "        similarity = cosine_similarity([sentence], [sentence_vector])\n",
    "        # Append the similarity score to the list (accessing the first element as similarity returns a 2D array)\n",
    "        similarities.append(similarity[0][0])\n",
    "        \n",
    "    similarities[number] = -1\n",
    "\n",
    "    closest_sentence_idx = np.argmax(similarities)\n",
    "\n",
    "    # Closest sentence\n",
    "    closest_sentence = feature[closest_sentence_idx]\n",
    "    \n",
    "    target_sentence = closest_sentence\n",
    "    for index, row in main_df.iterrows():\n",
    "        sentence = row['Text']  # Replace 'Sentence' with your actual column name for sentences\n",
    "        label = row['Label']        # Replace 'Label' with your actual column name for labels\n",
    "\n",
    "        if sentence == target_sentence:\n",
    "            if label == 0:\n",
    "                label = \"-\"\n",
    "            else:\n",
    "                label = '+'\n",
    "            break\n",
    "\n",
    "    \n",
    "    print(f\"Main senetence: {feature[number]}.\")\n",
    "    print(f\"Closest sentence label: {label}\")\n",
    "    print(f\"Closest sentence: {closest_sentence}.\")\n",
    "    \n",
    "    return closest_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f04049",
   "metadata": {},
   "source": [
    "### 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a1da4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main senetence: it 's a tour de force , written and directed so quietly that it 's implosion rather than explosion you fear.\n",
      "Closest sentence label: -\n",
      "Closest sentence: a semi autobiographical film that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the caterer had anything to do with it.\n"
     ]
    }
   ],
   "source": [
    "first_similarities = find_similarity(first_sentence, x_train_vector, train_X, train_df, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198bb9f",
   "metadata": {},
   "source": [
    "### 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a5ecaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sentence = x_train_vector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce16abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main senetence: places a slightly believable love triangle in a difficult to swallow setting , and then disappointingly moves the story into the realm of an improbable thriller.\n",
      "Closest sentence label: -\n",
      "Closest sentence: the plan to make enough into an inspiring tale of survival wrapped in the heart pounding suspense of a stylish psychological thriller ' has flopped as surely as a souffl gone wrong.\n"
     ]
    }
   ],
   "source": [
    "second_similarities = find_similarity(second_sentence, x_train_vector, train_X, train_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "009f5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(X_train, y_train, X_dev, y_dev):\n",
    "    error_rates = {}\n",
    "\n",
    "    for k in range(1, 100, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        dev_predictions = knn.predict(X_dev)\n",
    "\n",
    "        accuracy = accuracy_score(y_dev, dev_predictions)\n",
    "\n",
    "        error_rate = 1 - accuracy\n",
    "\n",
    "        error_rates[k] = [error_rate, knn]\n",
    "   \n",
    "    min_key = min(error_rates, key=lambda k: error_rates[k][0])\n",
    "    print(f\"Best k: {min_key}, smallest dev error: {error_rates[min_key][0]}\")\n",
    "    \n",
    "    return error_rates[min_key][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc0850",
   "metadata": {},
   "source": [
    "### 2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46a6d05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 73, smallest dev error: 0.278\n"
     ]
    }
   ],
   "source": [
    "best_knn = knn_classifier(x_train_vector, train_y, x_dev_vector, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c9b02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_vector = sentence_vec(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bebf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = best_knn.predict(x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53aebd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pd.DataFrame(predicted, columns=['Text'])\n",
    "# predictions_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4e860",
   "metadata": {},
   "source": [
    "### 2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feaed117",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hot = train_df['Label']\n",
    "train_X_hot = train_df['Text']\n",
    "\n",
    "dev_y_hot = dev_df['Label']\n",
    "dev_X_hot = dev_df['Text']\n",
    "\n",
    "test_X_hot = test_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f3e65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and conversion to numerical feature vectors using Bag-of-Words\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "train_X_vectorized = vectorizer.fit_transform(train_X_hot)\n",
    "train_X_vectorized_dense = train_X_vectorized.toarray()\n",
    "\n",
    "dev_X_vectorized = vectorizer.transform(dev_df['Text'])\n",
    "# Convert the sparse matrix to a dense array\n",
    "dev_X_vectorized_dense = dev_X_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "504baaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['007', '10', '100', ..., 'zoolander', 'zucker', 'zwick'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn = knn_classifier(train_X_vectorized_dense, train_y_hot, dev_X_vectorized_dense, dev_y_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0447d",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from svector import svector\n",
    "\n",
    "def read_from(textfile):\n",
    "    for line in open(textfile):\n",
    "        label, words = line.strip().split(\"\\t\")\n",
    "        yield (1 if label==\"+\" else -1, words.split())\n",
    "    \n",
    "def test(devfile, model, word_vectors):\n",
    "    tot, err = 0, 0\n",
    "    for i, (label, words) in enumerate(read_from(devfile), 1): # note 1...|D|\n",
    "        sentence_embedding = make_vector(words, word_vectors)\n",
    "        prediction = np.dot(model, sentence_embedding)\n",
    "        err += label * prediction <= 0\n",
    "    return err/i  # i is |D| now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(words, word_vectors):\n",
    "    embeddings = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis = 0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_basic(trainfile, devfile, word_vectors, epochs=10):\n",
    "    t = time.time()\n",
    "    best_err = 1.\n",
    "    model = np.zeros(word_vectors.vector_size)\n",
    "    best_model = model.copy()\n",
    "    for it in range(1, epochs+1):\n",
    "        updates = 0\n",
    "        for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "            sent = make_vector(words, word_vectors)\n",
    "            if label * np.dot(model, sent) <= 0:\n",
    "                updates += 1\n",
    "                model += label * sent\n",
    "        dev_err = test(devfile, model, word_vectors)\n",
    "        if dev_err < best_err:\n",
    "            best_model = model.copy()\n",
    "            best_err = dev_err\n",
    "        print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "    print(\"best dev err %.1f%%, |w|=%d, time: %.1f secs\" % (best_err * 100, len(model), time.time() - t))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b6fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_basic('train.txt', 'dev.txt', wv, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_average(trainfile, devfile, word_vectors, epochs=10):\n",
    "    t = time.time()\n",
    "    best_err = 1.\n",
    "    model = np.zeros(word_vectors.vector_size)\n",
    "    avg_model = np.zeros(word_vectors.vector_size)\n",
    "    c = 1\n",
    "    for it in range(1, epochs+1):\n",
    "        updates = 0\n",
    "        for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "            sent = make_vector(words, word_vectors)\n",
    "            if label * np.dot(model, sent) <= 0:\n",
    "                updates += 1\n",
    "                update = label * sent\n",
    "                model += update\n",
    "                avg_model += c * update\n",
    "            c += 1\n",
    "        new_model = model - avg_model/c\n",
    "        dev_err = test(devfile, new_model, word_vectors)\n",
    "        if dev_err < best_err:\n",
    "            best_model = new_model.copy()\n",
    "            best_er = dev_err\n",
    "            \n",
    "        print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "    print(\"best dev err %.1f%%, |w|=%d, time: %.1f secs\" % (best_er * 100, len(best_model), time.time() - t))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart average perceptron\n",
    "model = smart_average('train.txt', 'dev.txt', wv, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(trainfile, devfile, epochs=5):\n",
    "#     t = time.time()\n",
    "#     best_err = 1.\n",
    "#     model = svector()\n",
    "#     for it in range(1, epochs+1):\n",
    "#         updates = 0\n",
    "#         for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "#             sent = make_vector(words)\n",
    "#             if label * (model.dot(sent)) <= 0:\n",
    "#                 updates += 1\n",
    "#                 model += label * sent\n",
    "#         dev_err = test(devfile, model)\n",
    "#         best_err = min(best_err, dev_err)\n",
    "#         print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "#     print(\"best dev err %.1f%%, |w|=%d, time: %.1f secs\" % (best_err * 100, len(model), time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0121c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt.predicted', 'w') as f:\n",
    "    for i, (label, words) in enumerate(read_from(\"test.txt\"), 1):\n",
    "        f.write(\"%s\\t%s\\n\" % (\"+\" if np.dot(model, make_vector(words, wv)) > 0 else \"-\", \" \".join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c959d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb2a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2124f9",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2324f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_final = train_df['Label']\n",
    "train_X_final = train_df['Text']\n",
    "\n",
    "dev_y_final = dev_df['Label']\n",
    "dev_X_final = dev_df['Text']\n",
    "\n",
    "test_X_final = test_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to numerical data\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "train_X_final_vectors = tfidf_vectorizer.fit_transform(train_X_final)\n",
    "\n",
    "dev_X_final_vectors = tfidf_vectorizer.transform(dev_X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a652b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train_X_final_vectors, train_y_final)\n",
    "\n",
    "logistic_predictions = logistic_model.predict(dev_X_final_vectors)\n",
    "\n",
    "logistic_accuracy = accuracy_score(dev_y_final, logistic_predictions)\n",
    "\n",
    "logistic_error = 1 - logistic_accuracy\n",
    "\n",
    "end_time = time.time()\n",
    "logistic_runtime = end_time - start_time\n",
    "print(f'Logistic Regression dev error: {logistic_error}')\n",
    "print(f'Logistic Regression runtime: {logistic_runtime} seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(train_X_final_vectors, train_y_final)\n",
    "\n",
    "tree_predictions = tree_model.predict(dev_X_final_vectors)\n",
    "\n",
    "tree_accuracy = accuracy_score(dev_y_final, tree_predictions)\n",
    "\n",
    "tree_error = 1 - tree_accuracy\n",
    "\n",
    "end_time = time.time()\n",
    "tree_runtime = end_time - start_time\n",
    "print(f'Decision Tree dev error: {tree_error}')\n",
    "print(f'Decision Tree runtime: {tree_runtime} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51980d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f873a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
